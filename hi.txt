train.py

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from models import TreatmentModel
from preprocessing import preprocess_data
from sklearn.model_selection import ParameterSampler

def train_models(data_path, param_space, treatment_decisions, num_trials=10, num_epochs=100):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    try:
        # Preprocess the data
        balanced_data = preprocess_data(data_path)

        # Define the hyperparameter search space
        param_space = {
            'hidden_sizes': [(128, 64), (256, 128), (512, 256)],
            'learning_rate': [0.001, 0.01, 0.1],
        }

        # Perform random search
        best_models = {}
        for decision in treatment_decisions:
            best_loss = float('inf')
            best_model = None

            # Define input and output sizes based on the dataset
            input_size = balanced_data.drop([decision], axis=1).shape[1] 
            output_size = 1  # Binary survival outcome

            for _ in range(num_trials):
                # Sample hyperparameters
                params = list(ParameterSampler(param_space, n_iter=1))[0]

                # Train the model with the sampled hyperparameters
                model = TreatmentModel(input_size, params['hidden_sizes'], output_size).to(device)

                

                # Split the balanced data into training and testing sets
                X_train, X_test, y_train, y_test = train_test_split(balanced_data.drop([decision], axis=1),
                                                                    balanced_data[decision],
                                                                    test_size=0.2, random_state=42)

                # Convert data to PyTorch tensors
                X_train = torch.tensor(X_train.values, dtype=torch.float32).to(device)
                y_train = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1).to(device)

                # Train the model
                criterion = nn.BCEWithLogitsLoss()
                optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])

                for epoch in range(num_epochs):
                    optimizer.zero_grad()
                    outputs = model(X_train)
                    loss = criterion(outputs, y_train)
                    loss.backward()
                    optimizer.step()

                # Save the trained model
                torch.save(model.state_dict(), f'model_{decision}.pth')

                # Update the best model if the current model has a lower loss
                if loss < best_loss:
                    best_loss = loss
                    best_model = model

            best_models[decision] = best_model

        return best_models

    except RuntimeError as e:
        print(f"Error: {e}")
        print("Possible reasons:")
        print("1. Mismatch between the input data and the model's input size.")
        print("2. Insufficient memory to allocate tensors.")
    except Exception as e:
        print(f"Error: An unexpected error occurred during training: {e}")
        raise

models.py

import torch
import torch.nn as nn

class TreatmentModel(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        super(TreatmentModel, self).__init__()
        self.hidden_layers = nn.ModuleList()
        prev_size = input_size

        for hidden_size in hidden_sizes:
            self.hidden_layers.append(nn.Linear(prev_size, hidden_size))
            self.hidden_layers.append(nn.ReLU())
            self.hidden_layers.append(nn.BatchNorm1d(hidden_size))
            prev_size = hidden_size

        self.output_layer = nn.Linear(prev_size, output_size)

    def forward(self, x):
        for layer in self.hidden_layers:
            x = layer(x)
        x = self.output_layer(x)
        return x


evaluate.py

import torch
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from models import TreatmentModel
from preprocessing import preprocess_data
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit
from joblib import dump, load
import numpy as np
import pandas as pd
import os
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score
from sklearn.ensemble import RandomForestClassifier

def build_treatment_simulator(data_path, treatment_decisions, outcome_vars):
    try:
        # Preprocess the data
        balanced_data = preprocess_data(data_path)

        # Create a dictionary to store the SVC models for each outcome variable
        ts_models = {}

        for outcome in outcome_vars:
            model_path = f'ts_model_{outcome}.pkl'
            if os.path.exists(model_path):
            # Load the trained model if it exists
                ts_models[outcome] = load(model_path)
                print(f"Loaded trained Treatment Simulator model for {outcome}")
        else:
            print(f"Building Treatment Simulator model for {outcome}")

            # Define the parameter grid for GridSearchCV
            param_grid = {
                'C': [0.1, 1, 10],
                'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
                'degree': [2, 3, 4],
                'gamma': ['scale', 'auto'],
                'class_weight': ['balanced', None]
            }

            # Create StratifiedShuffleSplit for cross-validation
            cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)

            # Create the SVC model
            svc = SVC()

            # Perform grid search with cross-validation
            grid_search = GridSearchCV(svc, param_grid, cv=cv, n_jobs=-1, verbose=2)

            # Fit the grid search model
            grid_search.fit(balanced_data.drop(outcome, axis=1), balanced_data[outcome])

            # Get the best model
            best_model = grid_search.best_estimator_

            # Save the best model for the current outcome variable
            dump(best_model, model_path)
            ts_models[outcome] = best_model

        return ts_models

    except ValueError as e:
        print(f"Error: {e}")
        print("Possible reasons:")
        print("1. Mismatch between the input data and the expected format.")
        print("2. Invalid parameter values in the grid search.")
    except Exception as e:
        print(f"Error: An unexpected error occurred during Treatment Simulator building: {e}")
        raise

def evaluate_treatment_simulator(data_path, treatment_decisions, ts_models, num_bootstraps=1000):
    try:
        # Preprocess the data
        balanced_data = preprocess_data(data_path)

        # Initialize a dictionary to store the evaluation results
        evaluation_results = {}

        for outcome, model in ts_models.items():
            print(f"Evaluating Treatment Simulator model for {outcome}")

            # Initialize lists to store accuracy scores
            accuracy_scores = []

            # Perform bootstrapping
            for _ in range(num_bootstraps):
                # Perform stratified bootstrapping
                bootstrap_indices = np.random.choice(len(balanced_data), size=len(balanced_data), replace=True)
                bootstrap_data = balanced_data.iloc[bootstrap_indices]

                # Split the bootstrap data into features and target
                X_bootstrap = bootstrap_data.drop(outcome, axis=1)
                y_bootstrap = bootstrap_data[outcome]

                # Make predictions using the Treatment Simulator model
                y_pred = model.predict(X_bootstrap)

                # Calculate the accuracy score
                accuracy = accuracy_score(y_bootstrap, y_pred)
                accuracy_scores.append(accuracy)

            # Calculate the mean and 95% confidence interval for accuracy
            mean_accuracy = np.mean(accuracy_scores)
            ci_lower = np.percentile(accuracy_scores, 2.5)
            ci_upper = np.percentile(accuracy_scores, 97.5)

            # Print the evaluation results for the current outcome variable
            print(f"Outcome: {outcome}")
            print(f"Mean Accuracy: {mean_accuracy:.4f}")
            print(f"95% Confidence Interval: [{ci_lower:.4f}, {ci_upper:.4f}]")

            # Store the evaluation results
            evaluation_results[outcome] = {
                'Mean Accuracy': mean_accuracy,
                '95% CI Lower': ci_lower,
                '95% CI Upper': ci_upper
            }

        return evaluation_results

    except KeyError as e:
        print(f"Error: {e}")
        print("Possible reasons:")
        print("1. Mismatch between the input data and the expected format.")
        print("2. Incorrect column names or missing columns.")
    except Exception as e:
        print(f"Error: An unexpected error occurred during Treatment Simulator evaluation: {e}")
        raise

def evaluate_dql_models(data_path, treatment_decisions):

    try:
        # Preprocess the data
        balanced_data = preprocess_data(data_path)

        # Define input and output sizes based on the dataset
        input_size = balanced_data.shape[1] - 1  # Exclude the target variable
        output_size = 1  # Binary survival outcome

        models = {}
        for decision in treatment_decisions:
            model_path = f'model_{decision}.pth'
            if os.path.exists(model_path):
                # Load the saved model's state dictionary
                state_dict = torch.load(model_path)

                # Extract the hidden sizes from the saved model
                hidden_sizes = [state_dict['hidden_layers.0.weight'].shape[0], state_dict['hidden_layers.3.weight'].shape[0]]

                # Create a new model with the same architecture as the saved model
                model = TreatmentModel(input_size, hidden_sizes, output_size)
                model.load_state_dict(state_dict)
                models[decision] = model
            else:
                print(f"Trained model not found for {decision}. Skipping evaluation.")

        # Evaluate the models
        evaluation_results = {}

        for decision in treatment_decisions:
            print(f"Evaluating DQL model for {decision}")

            # Prepare the test data
            X_test = torch.tensor(balanced_data.drop([decision], axis=1).values, dtype=torch.float32)
            y_test = balanced_data[decision].values

            with torch.no_grad():
                y_pred = models[decision](X_test).squeeze().numpy()

            # Compare DQL model's decisions with physician's decisions
            physician_decisions = balanced_data[decision].values
            agreement = accuracy_score(physician_decisions, y_pred.round())
            kappa = cohen_kappa_score(physician_decisions, y_pred.round())

            # Calculate evaluation metrics
            accuracy = accuracy_score(y_test, y_pred.round())
            precision = precision_score(y_test, y_pred.round(), average='micro')
            recall = recall_score(y_test, y_pred.round(), average='micro')
            f1 = f1_score(y_test, y_pred.round(), average='micro')

            # Store the evaluation results
            evaluation_results[decision] = {
                'Accuracy': accuracy,
                'Precision': precision,
                'Recall': recall,
                'F1-score': f1,
                'Agreement': agreement,
                'Kappa': kappa
            }

            print(f"Evaluation metrics for {decision}:")
            print(f"Accuracy: {accuracy:.4f}")
            print(f"Precision: {precision:.4f}")
            print(f"Recall: {recall:.4f}")
            print(f"F1-score: {f1:.4f}")

        return evaluation_results

    except FileNotFoundError as e:
        print(f"Error: {e}")
        print("Possible reasons:")
        print("1. The trained model files are missing.")
        print("2. Incorrect file paths or names.")
        return None
    except RuntimeError as e:
        print(f"Error: {e}")
        print("Possible reasons:")
        print("1. Mismatch between the input data and the model's input size.")
        print("2. Insufficient memory to allocate tensors.")
        return None
    except Exception as e:
        print(f"Error: An unexpected error occurred during DQL model evaluation: {e}")
        return None

def analyze_feature_importance(X_test, y_test):
    model = RandomForestClassifier(random_state=42)
    model.fit(X_test, y_test)
    feature_importances = model.feature_importances_
    feature_names = X_test.columns.tolist()
    importance_dict = {feature: importance for feature, importance in zip(feature_names, feature_importances)}
    sorted_importances = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)
    return sorted_importances

if __name__ == "__main__":
    data_path = "dataset.csv"
    treatment_decisions = ['surgery', 'chemotherapy', 'hormone_therapy', 'radiotherapy']
    outcome_vars = ['OS', 'RFS', 'DFS']
    hidden_sizes = [128, 64]

    # Build and evaluate the Treatment Simulator models
    ts_models = build_treatment_simulator(data_path, treatment_decisions, outcome_vars)
    ts_evaluation_results = evaluate_treatment_simulator(data_path, treatment_decisions, ts_models)
    print("Treatment Simulator evaluation completed.")

    # Evaluate the DQL models
    dql_evaluation_results = evaluate_dql_models(data_path, treatment_decisions, hidden_sizes)
    print("DQL model evaluation completed.")


preprocessing.py

import pandas as pd
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from imblearn.over_sampling import RandomOverSampler
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.feature_selection import SelectKBest, f_classif

def preprocess_data(data_path, sample_size=None, random_state=42):
    try:
        # Load the data
        data = pd.read_csv(data_path)

        if sample_size is not None:
            # Perform stratified sampling based on all outcome variables
            outcome_vars = ['OS', 'RFS', 'DFS']
            stratify_data = data[outcome_vars].apply(lambda x: ''.join(x.astype(str)), axis=1)
            split = StratifiedShuffleSplit(n_splits=1, test_size=sample_size, random_state=random_state)
            for _, sample_indices in split.split(data, stratify_data):
                data = data.loc[sample_indices]

        
        # Remove constant features
        constant_columns = data.columns[data.nunique() <= 1]
        data = data.drop(columns=constant_columns)
                
        # Handle missing data
        data.fillna(data.mode().iloc[0], inplace=True)  # Categorical variables

        # Encode categorical variables
        label_encoder = LabelEncoder()
        categorical_columns = data.select_dtypes(include=['object']).columns
        for col in categorical_columns:
            data[col] = label_encoder.fit_transform(data[col])

        # Scale the features
        scaler = MinMaxScaler(feature_range=(-1, 1))
        data[data.columns] = scaler.fit_transform(data[data.columns])

        # Handle class imbalance
        outcome_vars = ['OS', 'RFS', 'DFS']
        original_columns = data.columns
        for outcome in outcome_vars:
            # print(f"Current columns in data: {data.columns}")
            data_copy = data.copy() 
            X = data_copy.drop(outcome, axis=1)  # Features
            y = data_copy[outcome]
            oversampler = RandomOverSampler(random_state=random_state)
            X_resampled, y_resampled = oversampler.fit_resample(X, y)
            data_copy = pd.concat([X_resampled, y_resampled], axis=1)

        # Reorder columns to match the original order
        data_copy = data_copy[original_columns]

    except FileNotFoundError:
        print(f"Error: The file '{data_path}' could not be found.")
        raise
    except KeyError as e:
        print(f"Error: The column '{e.args[0]}' does not exist in the dataset.")
        raise
    except Exception as e:
        print(f"Error: An unexpected error occurred during preprocessing: {e}")
        raise

    return data_copy

def select_features(data, outcome_vars, k=10):
    X = data.drop(outcome_vars, axis=1)  # Features
    selected_features = {}

    for outcome in outcome_vars:
        if outcome not in data.columns:
            raise KeyError(f"The column '{outcome}' does not exist in the dataset.")

        y = data[outcome]  # Target variable

        selector = SelectKBest(score_func=f_classif, k=k)
        selector.fit(X, y)

        selected_columns = X.columns[selector.get_support()].tolist()
        selected_features[outcome] = selected_columns

    return selected_features

if __name__ == "__main__":
    data_path = "dataset.csv"
    sample_size = 10000
    processed_data = preprocess_data(data_path, sample_size=sample_size)
    print("Data preprocessing completed.")